\documentclass[11pt,a4paper]{google}

\usepackage[authoryear,sort&compress,round]{natbib}
\bibliographystyle{abbrvnat}

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{verbatim}
\usepackage{xurl}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes.geometric}

\keywords{EEG, motor imagery, LaBraM, EEGNet, transfer learning, curriculum learning, BCI, reproducibility, negative results}

\title{EEG Motor Imagery with Foundation-Model Transfer and Compact CNN Baselines:\\
Comprehensive Consolidation of LaBraM Probing/Fine-Tuning/Curriculum and EEGNet Training/Tuning}

\author[1]{Prannaya Gupta}
\author[2]{Codex}
\author[3]{Rage Against The Machine Team}
\affil[1]{Independent Researcher}
\affil[2]{AI Research Assistant}
\affil[3]{RATM Project Team}

\correspondingauthor{prannaya.gupta@placeholder.edu}
\paperurl{https://github.com/your-org/your-repo}
\reportnumber{RATM-EEG-2026-02}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}

\lstdefinestyle{py}{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue!70!black},
  commentstyle=\color{green!35!black},
  stringstyle=\color{red!60!black},
  breaklines=true,
  showstringspaces=false,
  frame=single,
  tabsize=2
}

\newcommand{\ceThree}{\ensuremath{\log 3 \approx 1.0986}}
\newcommand{\ceTwo}{\ensuremath{\log 2 \approx 0.6931}}

\begin{abstract}
This manuscript consolidates the full EEG motor-imagery (MI) research trail implemented in \texttt{ml/}, including script code and notebook experiments (\texttt{ml/labram-for-eegmmidb.ipynb}). We systematize two modeling tracks: (i) LaBraM transfer experiments with probing, partial/full fine-tuning, and two-stage curriculum training; and (ii) EEGNet/EEGNetResidual compact baselines with subsequent user-level Muse calibration tuning. The cross-subject PhysioNet split used subjects 1--40 for training and 41--50 for validation, with runs [4, 8, 12]. Consolidated evidence shows persistent class-collapse dynamics for several LaBraM settings, especially in 3-class mode (left/right/rest), where completed runs plateaued at 0.5000 validation accuracy near \ceThree. Binary LaBraM improved modestly, with the best completed run reaching 0.5556. A full LaBraM fine-tune run reached train accuracy 0.9966 but only 0.5222 validation accuracy, indicating severe overfitting. The curriculum run improved checkpoint selection robustness (best MCC 0.0753; best val\_acc 0.5378; best balanced accuracy 0.5339) without surpassing the best binary accuracy. The compact EEGNetResidual baseline achieved 0.5919 on the cross-subject split. Muse personalization in current script defaults trains on all data (no holdout); when a holdout is enforced (\texttt{--no-use-all-data}), best validation accuracy is 0.5918 (epoch 82, 247 total epochs, 49 validation epochs). We provide formal propositions/proofs for diagnostic metrics (chance-level cross-entropy, collapse-accuracy bounds, MCC degeneracy for constant predictors, token-budget bounds in LaBraM preprocessing), exploratory code listings, and a reproducible run ledger suitable for critical academic audit.
\end{abstract}

\begin{document}
\maketitle

\section{Introduction}
EEG MI decoding remains difficult because discriminative information is weak relative to noise, highly nonstationary across sessions and subjects, and sensitive to preprocessing and data partitioning \citep{lawhern2018eegnet,schirrmeister2017deep}. Foundation-model (FM) transfer is attractive in principle: pre-train large representations once, then adapt to downstream tasks. In practice, reliable transfer requires that the downstream optimization interface preserve class-separable geometry under realistic channel and data constraints.

This work answers a practical question: \emph{what does the current project actually demonstrate when all \texttt{ml/} artifacts are jointly considered}? The previous report did not fully integrate notebook and script evidence. Here we consolidate these artifacts into one reproducible, technically defensible record.

\section{Consolidated Artifact Map}
Table~\ref{tab:artifact-map} maps code artifacts to scientific roles.

\begin{longtable}{@{}p{0.30\linewidth}p{0.65\linewidth}@{}}
\caption{Consolidated artifact map for \texttt{ml/} and notebook work.}\label{tab:artifact-map}\\
\toprule
Artifact & Role in this study \\
\midrule
\endfirsthead
\toprule
Artifact & Role in this study \\
\midrule
\endhead
\texttt{ml/eeg/dataset.py} & PhysioNet acquisition, event filtering, epoch creation, band-pass preprocessing. \\
\texttt{ml/models/labram\_probe.py} & Script-level LaBraM probe with optional block unfreezing and token pooling. \\
\texttt{ml/labram\_probe.py} & Reproducible LaBraM training entrypoint over subject-level split. \\
\texttt{ml/labram-for-eegmmidb.ipynb} & Main exploratory run trail: 3-class, binary, full fine-tune, curriculum, diagnostics, and embedding geometry analysis. \\
\texttt{ml/training\_notebook\_cell.py} & Scripted extraction of the notebook training logic: stage switch, MCC selection, per-class diagnostics, scheduler behavior. \\
\texttt{ml/eeg/layers/labram\_encoder.py} & Tokenization interface to pre-trained LaBraM backbone, including patch budget truncation rule. \\
\texttt{ml/eeg/layers/labram/neural\_transformer.py} & Backbone internals (patch size, depth, embedding path, class token). \\
\texttt{ml/\_\_main\_\_.py} & Cross-subject EEGNet/EEGNetResidual baseline training flow and summary logging. \\
\texttt{ml/tune\_eegnet\_muse.py} & User-level Muse fine-tuning script; includes all-data mode and optional holdout mode. \\
\texttt{docs/eeg/appendix\_labram\_raw\_logs.tex} & Verbatim log archive used for auditability and traceback preservation. \\
\bottomrule
\end{longtable}

\section{Materials and Methods}
\subsection{Datasets and Splits}
\textbf{Cross-subject PhysioNet MI setup.}
Runs [4, 8, 12] were used with events \texttt{T0}, \texttt{T1}, \texttt{T2} (rest, left, right), following the code configuration.
Observed consolidated split statistics:
\begin{itemize}
\item 3-class train: 3510 epochs, shape \((3510,64,601)\), counts \([884,871,1755]\).
\item 3-class val: 900 epochs, shape \((900,64,601)\), counts \([229,221,450]\).
\item Binary derivation (\texttt{T1}/\texttt{T2} only): train \([884,871]\), val \([229,221]\), totals 1755/450.
\end{itemize}

\textbf{Muse calibration setup.}
Current discovered calibration files:
\texttt{left\_muse\_v1,v2,v3.csv} and \texttt{right\_muse\_v1,v2,v3.csv} (6 files total).
Epoch counts (3-second windows) sum to 247 samples. Holdout mode with \texttt{val\_split=0.2} gives 198 training and 49 validation samples.

\subsection{Signal Processing}
Both tracks apply 8--30 Hz band-pass filtering (5th-order Butterworth in code).
Let \(x_{i,c}(t)\) be epoch \(i\), channel \(c\). Filtered signal is
\begin{equation}
\tilde{x}_{i,c}(t) = (h * x_{i,c})(t),
\end{equation}
with \(h\) induced by \((\text{lowcut},\text{highcut})=(8,30)\) Hz.

For LaBraM experiments, optional long-context interpolation resamples epochs to length \(L'=1600\):
\begin{equation}
\hat{x}_{i,c} = \mathcal{R}_{L'}(\tilde{x}_{i,c}),
\end{equation}
followed by per-epoch/channel z-score normalization.

For EEGNet tuning, Muse epochs are resampled to the checkpoint input length (\(n\_samples=481\)).

\subsection{Modeling Tracks}
\textbf{LaBraM transfer track.}
The notebook explored:
\begin{itemize}
\item frozen/probe-like training,
\item partial unfreezing of final transformer blocks,
\item full fine-tuning (\texttt{full\_finetune=True}),
\item curriculum: stage-1 head-only then stage-2 unfreeze-last-2 blocks, with MCC-based checkpoint selection.
\end{itemize}
The backbone in code uses \(T=\)200 patch size, \(d=\)200 embedding dimension, depth 12.

\textbf{EEGNet track.}
\begin{itemize}
\item Cross-subject baseline: \texttt{ml/\_\_main\_\_.py} with EEGNetResidual, 4 channels, best val accuracy 0.5919 (log excerpt in appendix).
\item Later tuning: \texttt{ml/tune\_eegnet\_muse.py} on user calibration CSVs.
\end{itemize}

\subsection{Optimization Objective}
Main classification objective:
\begin{equation}
\mathcal{L}_{\text{CE}} = -\frac{1}{B}\sum_{i=1}^{B}\log p_\theta(y_i|x_i).
\end{equation}
Some notebook runs used label smoothing (\(\epsilon=0.05\)); curriculum selection used MCC (binary) or balanced accuracy fallback.

\section{Formal Diagnostic Propositions and Proofs}
\subsection{Chance-level Cross-Entropy Baseline}
\begin{proposition}
For \(K\)-class classification with uniform prediction \(p(y=k|x)=1/K\), expected cross-entropy equals \(\log K\).
\end{proposition}
\begin{proof}
For any ground-truth class \(y\), loss is \(-\log p(y|x) = -\log(1/K)=\log K\). Expectation over data leaves \(\log K\) unchanged.
\end{proof}

\begin{corollary}
Random-guess baselines here are \ceThree\ for 3-class and \ceTwo\ for binary.
\end{corollary}

\subsection{Collapse Accuracy Bound}
\begin{proposition}
Let class priors be \(\pi_1,\dots,\pi_K\), \(\sum_j \pi_j=1\). If a classifier collapses to constant prediction \(\hat{y}=j^\star\), then accuracy is \(\pi_{j^\star}\). Best possible constant-prediction accuracy is \(\max_j \pi_j\).
\end{proposition}
\begin{proof}
Accuracy is \(\Pr(\hat{y}=y)=\Pr(y=j^\star)=\pi_{j^\star}\). Maximizing over \(j^\star\) yields \(\max_j \pi_j\).
\end{proof}

\begin{corollary}
For 3-class validation counts \([229,221,450]\), \(\max_j \pi_j = 450/900 = 0.5\). Thus persistent 0.5000 accuracy is compatible with majority-class collapse.
\end{corollary}

\subsection{MCC Degeneracy Under Constant Binary Prediction}
\begin{proposition}
Assume both binary classes appear in ground truth. If predictions are constant (\(\hat{y}\equiv 0\) or \(\hat{y}\equiv 1\)), then MCC is 0 under the standard safe-convention used in code (return 0 when denominator is 0).
\end{proposition}
\begin{proof}
MCC:
\[
\mathrm{MCC}=\frac{TP\cdot TN-FP\cdot FN}
{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}.
\]
For constant prediction, at least one confusion-matrix marginal in the denominator is zero, yielding denominator 0. The implemented function returns 0 in this case. Therefore MCC = 0.
\end{proof}

\begin{corollary}
In the binary validation split \([229,221]\), predicting all class 0 gives accuracy \(229/450=0.5089\), balanced accuracy 0.5, MCC 0.
\end{corollary}

\subsection{LaBraM Token Budget Bound}
\begin{proposition}
Given input sequence length \(SL\) and patch size \(T=200\), the implemented LaBraM encoder uses
\[
P = \min\!\left(\left\lfloor \frac{SL}{T}\right\rfloor,16\right)
\]
patches per channel. Patch-token count is \(N\cdot P\); with class token it is \(N\cdot P + 1\).
\end{proposition}
\begin{proof}
Directly from \texttt{ml/eeg/layers/labram\_encoder.py}: \(P\) is computed as above, the input is sliced to the last \(P\cdot T\) samples, then reshaped to \((B,N,P,T)\). The transformer appends one class token before attention.
\end{proof}

\begin{corollary}
Without long-context resampling, \(SL=601\Rightarrow P=3\). With resampling to 1600, \(P=8\). Hence long-context mode increases per-channel patch count from 3 to 8.
\end{corollary}

\section{Experimental Ledger}
\subsection{LaBraM Notebook Run Ledger}
Table~\ref{tab:labram-ledger} separates completed and interrupted runs.

\begin{longtable}{@{}p{0.07\linewidth}p{0.16\linewidth}p{0.17\linewidth}p{0.22\linewidth}p{0.14\linewidth}p{0.18\linewidth}@{}}
\caption{Consolidated LaBraM run ledger from \texttt{ml/labram-for-eegmmidb.ipynb}.}\label{tab:labram-ledger}\\
\toprule
Cell & Task & Core setup & Selection/diagnostics & Status & Best reported result \\
\midrule
\endfirsthead
\toprule
Cell & Task & Core setup & Selection/diagnostics & Status & Best reported result \\
\midrule
\endhead
10 & 3-class & Baseline-style training & Standard val acc & Completed & val\_acc 0.5000, early stop epoch 44 \\
12 & 3-class & Unfreeze-last-2, detailed per-class logs & Pred-count and per-class acc each epoch & Completed & val\_acc 0.5000, early stop epoch 41 \\
11 & 3-class & Partial unfreeze run & Standard diagnostics & Interrupted & best seen val\_acc 0.2543 (epoch 3) \\
13 & 3-class & CLS pooling, freeze config variant & Includes sanity probe & Interrupted & best seen val\_acc 0.4944 (sanity 0.5022) \\
7 & Binary & Full fine-tune; long-context 1600; label smoothing 0.05; CLS pooling & Frozen-embedding sanity probe & Completed & val\_acc 0.5222 (sanity 0.5444), early stop epoch 46 \\
8 & Binary & Curriculum: stage-1 head-only (12 epochs) then unfreeze last 2 blocks & MCC-based selection, per-class diagnostics & Completed & val\_acc 0.5378, bal\_acc 0.5339, MCC 0.0753, early stop epoch 53 (sanity 0.5600) \\
14 & Binary & Unfreeze-last-2, no long-context variant & Standard diagnostics & Interrupted & best seen val\_acc 0.5400 (epoch 74) \\
15 & Binary & Unfreeze-last-2 + long-context + label smoothing 0.05 & Standard diagnostics & Completed & val\_acc 0.5556, early stop epoch 83 \\
\bottomrule
\end{longtable}

\subsection{EEGNet Baseline and Tuning Ledger}
\begin{longtable}{@{}p{0.20\linewidth}p{0.33\linewidth}p{0.20\linewidth}p{0.22\linewidth}@{}}
\caption{EEGNet/EEGNetResidual results consolidated from script logs and direct reruns.}\label{tab:eegnet-ledger}\\
\toprule
Run & Data regime & Key setting & Outcome \\
\midrule
\endfirsthead
\toprule
Run & Data regime & Key setting & Outcome \\
\midrule
\endhead
Cross-subject EEGNetResidual baseline (\texttt{ml/\_\_main\_\_.py}) & Train 2925 / Val 750, shape \((4,481)\), class counts train \([1470,1455]\), val \([379,371]\) & Early-stop patience 200, CPU log excerpt archived & Best val\_acc 0.5919 at epoch 73; stop at epoch 273 \\
Muse tuning holdout (\texttt{tune\_eegnet\_muse.py --no-use-all-data}) & 6 CSVs, total 247 epochs, split 198/49 & Freeze early layers (\texttt{conv1}, \texttt{batchnorm1}), 200 epochs & Best val\_acc 0.5918 at epoch 82 \\
Muse tuning default (\texttt{tune\_eegnet\_muse.py}) & 6 CSVs, all 247 epochs in train & \texttt{--use-all-data} default enabled, no validation split & Best train loss 0.4539 at epoch 196; no validation accuracy reported \\
\bottomrule
\end{longtable}

\section{Results}
\subsection{LaBraM: Main Observations}
\textbf{3-class regime.}
Completed 3-class runs repeatedly converged to 0.5000 validation accuracy with strong collapse/flip behavior in prediction histograms (e.g., all predictions in one class, then another), consistent with Proposition 2 and corollary bounds.

\textbf{Binary regime.}
Best completed binary result was 0.5556 (cell 15), approximately +4.67 percentage points above majority-class collapse (0.5089), but still far below robust decoding thresholds typically expected for stable BCI control.

\textbf{Overfitting in full fine-tune (cell 7).}
Train accuracy increased to 0.9966 (epoch 42), while validation accuracy peaked at 0.5222 and validation loss rose above 1.5, indicating memorization without generalization.

\textbf{Curriculum behavior (cell 8).}
Stage switch at epoch 13 improved checkpoint selection quality using MCC (best 0.0753). This improved metric robustness but did not exceed the best binary accuracy obtained in cell 15.

\subsection{Embedding Geometry Probe}
From notebook exploratory analysis:
\begin{itemize}
\item PCA explained variance ratio: [0.19062445, 0.10972243].
\item Silhouette (PCA 2D): 0.0099735.
\item Silhouette (t-SNE 2D): 0.0099501.
\end{itemize}
Near-zero silhouette values indicate weakly separated clusters in low-dimensional projections, consistent with observed collapse dynamics.

\subsection{EEGNet Track Interpretation}
The compact cross-subject EEGNetResidual baseline (0.5919) exceeds all completed LaBraM runs in this consolidated record (best LaBraM 0.5556). Muse tuning with enforced holdout produced 0.5918. Importantly, current script defaults do not produce a validation metric unless \texttt{--no-use-all-data} is set, which is critical for scientific reporting discipline.

\section{Exploratory Code Listings}
\subsection*{Listing 1: LaBraM token-budget rule (\texttt{ml/eeg/layers/labram\_encoder.py})}
\begin{lstlisting}[style=py]
T = self.model.patch_size
P = min(SL // T, 16)
x = x[:, :, -P*T:]
x = rearrange(x, "B N (P T) -> B N P T", T=T)
\end{lstlisting}

\subsection*{Listing 2: Curriculum + MCC logic (\texttt{ml/training\_notebook\_cell.py})}
\begin{lstlisting}[style=py]
def binary_mcc_from_tensors(y_pred, y_true):
    tp = int(((y_pred == 1) & (y_true == 1)).sum().item())
    tn = int(((y_pred == 0) & (y_true == 0)).sum().item())
    fp = int(((y_pred == 1) & (y_true == 0)).sum().item())
    fn = int(((y_pred == 0) & (y_true == 1)).sum().item())
    denom = np.sqrt(float((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))
    if denom == 0.0:
        return 0.0
    return float((tp * tn - fp * fn) / denom)

if stage2_enabled and epoch == stage2_start_epoch:
    model.configure_trainable_encoder(
        freeze_encoder=True,
        unfreeze_last_n_blocks=stage2_unfreeze_last_n_blocks,
    )
\end{lstlisting}

\subsection*{Listing 3: Muse split-mode guard (\texttt{ml/tune\_eegnet\_muse.py})}
\begin{lstlisting}[style=py]
if use_all_data:
    X_train, y_train = X, y
    X_val = np.empty((0, X.shape[1], X.shape[2]), dtype=X.dtype)
    y_val = np.empty((0,), dtype=y.dtype)
    print("Training mode: using all data (no validation split).")
else:
    X_train, y_train, X_val, y_val = _split_train_val(
        X, y, val_split=args.val_split, seed=args.seed
    )
    print(f"Training mode: train/val split with val_split={args.val_split:.2f}")
\end{lstlisting}

\section{Discussion}
The consolidated evidence supports a careful, publication-grade negative-to-mixed conclusion:
\begin{itemize}
\item In this project state, LaBraM transfer did not reliably surpass compact EEGNet baselines on cross-subject MI decoding.
\item Multiple LaBraM runs show collapse dynamics predicted by simple metric theory (Sections 4--6).
\item Curriculum learning improved checkpoint \emph{selection quality} (MCC), but not final best accuracy relative to the best binary run.
\item Muse personalization remains promising, but must be reported with explicit split protocol; all-data training cannot be treated as validation evidence.
\end{itemize}

These findings are still scientifically useful: they isolate where transfer currently fails (representation-task interface and optimization stability), rather than only reporting top-line numbers.

\section{Limitations and Threats to Validity}
\begin{itemize}
\item Several notebook runs were interrupted; they provide trajectory evidence, not completed endpoint claims.
\item No multi-seed inferential statistics are included yet.
\item Cross-subject and within-user regimes are different tasks; direct score comparison should be interpreted cautiously.
\item Muse holdout is random within captured sessions; external-session generalization remains untested.
\end{itemize}

\section{Conclusion}
After consolidating all available \texttt{ml/} artifacts, the current evidence shows: (i) frequent LaBraM class-collapse behavior in 3-class MI; (ii) modest binary improvements with best completed LaBraM val\_acc 0.5556; (iii) strong overfitting under full fine-tuning; (iv) improved selection stability but limited headline gain under curriculum learning; and (v) competitive compact CNN performance (0.5919 cross-subject baseline, 0.5918 Muse holdout tuning). The immediate research direction is not broader claim inflation, but rigorous stabilization: seed-robust protocols, stronger regularization, tighter calibration evaluation, and controlled comparisons against compact baselines under identical splits.

\section{Reproducibility and Availability}
\textbf{Code paths.}
Core scripts are in \texttt{ml/}; this manuscript references exact files listed in Table~\ref{tab:artifact-map}.

\textbf{Representative commands.}
\begin{verbatim}
python ml/labram_probe.py
python ml/tune_eegnet_muse.py --data-dir ml/calibration_data --no-use-all-data
python ml/tune_eegnet_muse.py --data-dir ml/calibration_data
\end{verbatim}

\textbf{Raw logs.}
Verbatim run excerpts and traceback archive are retained in Appendix~\ref{appendix:rawlogs}.

\section{Ethics and Safety}
This work is non-clinical and does not make medical claims. No personally identifying data are reported in this manuscript.

\bibliography{main}

\clearpage
\appendix
\section{Raw Experimental Logs and Error Trace}\label{appendix:rawlogs}
\input{appendix_labram_raw_logs}

\end{document}
